import streamlit as st
from gpt import bot
from streamlit_tags import st_tags
from sentence_formation import word_formater, multiple_word_formater
import random
import time
import spacy
import json
import re
from deta import Deta
from streamlit_extras.grid import grid
from dotenv import load_dotenv
import os
from input_field_component import input_field
from better_profanity import profanity
from check_existence import exist


#–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–π –º–æ–¥—É–ª—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–∞–≤–æ–ø–∏—Å–∞–Ω–∏—è

from spellchecker import SpellChecker
spell = SpellChecker()

#–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–Ω–µ—à–µ–Ω–≥–æ –≤–∏–¥–∞ —Å—Ä–∞–Ω–∏—Ü—ã, —Å–∫—Ä—ã—Ç–∏–µ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö —ç–µ–ª–µ–º–µ–Ω—Ç–æ–≤

st.set_page_config(
    page_title="LexisGen",
    page_icon="üìö",
    layout="wide",
    initial_sidebar_state="auto"
)

st.markdown("""
<style>
    div[data-testid="stToolbar"] { display: none;}
    footer {visibility: hidden;}
</style>
""", unsafe_allow_html=True)


load_dotenv()

#–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è

base_key = os.getenv('DATABASE_KEY')
deta = Deta(base_key)
database = deta.Base("users")


# –∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞–º–∏
@st.cache_data(show_spinner=False)
def load_contexts():
    with open('./contexts.json') as json_file:
        data = json.load(json_file)
    return data

contexts = load_contexts()

# –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è
@st.cache_resource
def load_dictionary(dict_type):
    nlp = spacy.load(dict_type)
    return nlp

nlp = load_dictionary("en_core_web_sm")

#–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç, —Ñ–ª–∞–≥–æ–≤ –∏ –∫–æ–ª–ª–µ–∫—Ü–∏–π –¥–∞–Ω–Ω—ã—Ö
main_prompt = '''You are provided with a list of words and contexts, and you should generate a list of sentences at elementary English level, where the mentioned words are used in the appropriate context. When making sentences, follow these requirements:
1. Leave the forms and tenses of the words unchenged.
2. You have to provide only one sentence for each word and nothing else.
3. These words are not replaced with synonyms.
4. Each sentence must contain at least 20 words.'''

context_promt = '''You provided with a list of words and phrases. For each word or phras from the list create new list of 10 contexts where that word or phrase can be used, do not add examples and any notes, provide only lists with names of contexts, describe each context with single word
Generate list in the certain way:

1. first word:
-context
-context\n\n
2. second word:
-context
-context'''

if "define_initial_values" not in st.session_state:
    st.session_state.define_initial_values = True

if "data" not in st.session_state:
    st.session_state.data = [{"role": "system", "content": main_prompt}]

if "context_data" not in st.session_state:
    st.session_state.context_data = [{"role": "system", "content": context_promt}]

if "level" not in st.session_state:
    st.session_state.level = "middle"

if "stop_words" not in st.session_state:
    st.session_state.stop_words = ["a", "an", "not", "someone", "something", "somewhere", "to", "am", "are", "is", "the", "someones", "ones", "someone's", "one's"]

if "stop_words_v2" not in st.session_state:
    st.session_state.stop_words_v2 = ["a", "an", "to", "the"]

if "propriate_spell" not in st.session_state:
    st.session_state.propriate_spell = True

if "responses" not in st.session_state:
    st.session_state.responses = []

if "formated_responses" not in st.session_state:
    st.session_state.formated_responses = []

if "number_of_sentenses" not in st.session_state:
    st.session_state.number_of_sentenses = 0

if "answers" not in st.session_state:
    st.session_state.answers = []

if "unique_key" not in st.session_state:
    st.session_state.unique_key = 0

if "user_answers" not in st.session_state:
    st.session_state.user_answers = {}

if "check_answers" not in st.session_state:
    st.session_state.check_answers = False

if "check_language" not in st.session_state:
    st.session_state.check_language = True

if "not_in_sw" not in st.session_state:
    st.session_state.not_in_sw = True

if "key_is_provided" not in st.session_state:
    st.session_state.key_is_provided = False
    
if "tokens" not in st.session_state:
    st.session_state.tokens = 0

if "collections" not in st.session_state:
    st.session_state.collections = []

if "show_answers_form" not in st.session_state:
    st.session_state.show_answers_form = False

if "results" not in st.session_state:
    st.session_state.results = []

if "gens_number" not in st.session_state:
    st.session_state.gens_number = 0

if "show_submit_form" not in st.session_state:
    st.session_state.show_submit_form = False

if "allow_generation" not in st.session_state:
    st.session_state.allow_generation = False

if "error_item" not in st.session_state:
    st.session_state.error_item = ""

if "input_item" not in st.session_state:
    st.session_state.input_item = ""

if "initial_values" not in st.session_state:
    st.session_state.initial_values = []

if "acsses_key" not in st.session_state:
    st.session_state.acsses_key = ""

if "swearword" not in st.session_state:
    st.session_state.swearword = ""

if "suggestion" not in st.session_state:
    st.session_state.suggestion = ""

if "wordnotexist" not in st.session_state:
    st.session_state.wordnotexist = ""

if "notenought" not in st.session_state:
    st.session_state.notenought = ""

if "impossibleword" not in st.session_state:
    st.session_state.impossibleword = ""

if "propriate_spell" not in st.session_state:
    st.session_state.propriate_spell = True

if "propriate_length" not in st.session_state:
    st.session_state.propriate_length = True

if "profanity" not in st.session_state:
    st.session_state.profanity = False

if "max_phrase_lenght" not in st.session_state:
    st.session_state.max_phrase_lenght = True

if "too_long_item" not in st.session_state:
    st.session_state.too_long_item = ""

with st.sidebar:


    if acsses_key := st.text_input("–ö–ª—é—á –¥–æ—Å—Ç—É–ø–∞", type="password", autocomplete=""):
        if acsses_key != "":
            @st.cache_data()
            def check_acsses_key(token):
                if database.get(token) != None:

                    key_is_provided = True
                    return key_is_provided
                else:
                    key_is_provided = False
                    return key_is_provided
            st.session_state.key_is_provided = check_acsses_key(acsses_key)

            if st.session_state.key_is_provided:
                data = database.get(acsses_key)

                st.session_state.acsses_key = data["access_key"]


                st.session_state.gens_number = data["gens_number"]

            else:
                    st.warning("–ö–ª—é—á –¥–æ—Å—Ç—É–ø–∞ –Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω, –ª–∏–±–æ –≤–≤–µ–¥—ë–Ω –Ω–µ–≤–µ—Ä–Ω–æ")
        else:
            st.session_state.key_is_provided = False
    else:
        st.session_state.key_is_provided = False

main_grid = grid([2, 4.5, 2])

main_grid.write("")

with main_grid.container():

    st.markdown('<h1 style="text-align:center">LexisGen</h1>', unsafe_allow_html=True)

    st.write("")
    st.write("")

    with st.form("words input"):
        if "keywords" not in st.session_state:
            st.session_state.keywords = []

        text, maxtags = ('–ù–∞–∂–º–∏—Ç–µ Enter, —á—Ç–æ–±—ã –¥–æ–±–∞–≤–∏—Ç—å', 8)

        # st.subheader("–í–≤–µ–¥–∏—Ç–µ —Å–ª–æ–≤–∞:", anchor=False)
        st.info(' &nbsp; –ß—Ç–æ–±—ã —Å–æ–∑–¥–∞—Ç—å —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ, –≤–≤–µ–¥–∏—Ç–µ —Å–ª–æ–≤–∞ –∏–ª–∏ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏—è –≤ –ø–æ–ª–µ –Ω–∏–∂–µ –∏ –Ω–∞–∂–º–∏—Ç–µ "–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å"', icon="üåå")

        if keywords := st_tags(label="", text=text, value=[], maxtags = 8, key='keyword'): 
            st.session_state.keywords = keywords

            st.session_state.propriate_spell = True
            st.session_state.profanity = False
            st.session_state.max_phrase_lenght = True

        #–æ—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º—ã
        if st.form_submit_button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å"):
            with st.spinner('–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –≤–≤–µ–¥—ë–Ω–Ω—ã—Ö —Å–ª–æ–≤...'):
                for item in st.session_state.keywords:

                    #–ø—Ä–æ–≤–µ—Ä–∫–∞, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –∫ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º—É —è–∑—ã–∫—É

                    reg = re.compile(r'[a-zA-Z]')
                    if reg.match(item.lower()):
                        st.session_state.check_language = True
                    else:
                        st.session_state.check_language = False
                        st.session_state.input_item = item

                        break

                    # –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –≤–≤–µ–¥–µ–Ω–æ–≥–æ —Å–ª–æ–≤–∞ 
                    # –ï—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–∞–ø–∏—Å–∞–Ω–æ –Ω–µ–≤–µ—Ä–Ω–æ, —Ç–æ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç, –µ—Å–ª–∏ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –Ω–µ—Ç - —É–∫–∞–∑–∞—Ç—å, —á—Ç–æ —Ç–∞–∫–æ–≥–æ —Å–ª–æ–≤–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç

                    try:
                        item = item.lower().replace("someone's", "someone")
                        item = item.lower().replace("one's", "one")

                        item = item.lower().replace("someone‚Äôs", "someone")
                        item = item.lower().replace("one‚Äôs", "one")

                        temp = item.lower().replace('-', " ")
                        temp = temp.split()
                        misspelled = spell.unknown(temp)

                        for element in item.lower().strip().split():
                            if not exist(element):
                                st.session_state.propriate_spell = False
                                for w in misspelled:
                                    i = temp.index(w)
                                    temp[i] = spell.correction(w)
                            
                                st.session_state.wordnotexist = item
                                st.session_state.suggestion = " ".join(temp)
                                break
                    except:
                        st.session_state.propriate_spell = "impossible_word"
                        st.session_state.impossibleword = item

                        break


                    # –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–æ—Ä–∞–ª—å–Ω–æ-—ç—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å–≤–∏–µ

                    if profanity.contains_profanity(item.lower()):
                        st.session_state.profanity = True
                        st.session_state.swearword = item
                        break
                    #–ø—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –≤–≤–µ–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–ª–æ–≤–æ—á–µ—Ç–∞–Ω–∏–µ–º –∏–ª–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Å–ª–æ–≤–æ–º

                    if len(item.split()) > 1:
                        t = 0
                        for w in item.split():
                            if w.lower() in st.session_state.stop_words:
                                st.session_state.propriate_length = True
                                continue
                            else:
                                t += 1
                        if t > 5:
                            st.session_state.max_phrase_lenght = False
                            st.session_state.too_long_item = item

                            break

                    #–ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¢–æ–ª—å–∫–æ –∫ —Å–ø–∏—Å–∫—É —Å—Ç–æ–ø-—Å–ª–æ–≤
                    for element in item.lower().strip().split():
                        if element not in st.session_state.stop_words:
                            st.session_state.not_in_sw = True
                            break
                        else:
                            st.session_state.not_in_sw = False
                            st.session_state.notenought = item

                if st.session_state.max_phrase_lenght and st.session_state.propriate_spell == True and st.session_state.check_language and st.session_state.not_in_sw and len(st.session_state.keywords) >= 2 and st.session_state.gens_number > 0 and not st.session_state.profanity:
                    st.session_state.allow_generation = True
                else:
                    st.session_state.allow_generation = False

            if st.session_state.key_is_provided:
                if st.session_state.allow_generation:

                    st.session_state.responses.clear()
                    st.session_state.formated_responses.clear()
                    st.session_state.answers.clear()
                    st.session_state.user_answers.clear()
                    st.session_state.url_q = 0
                    st.session_state.check_answers = False
                    st.session_state.data = [{"role": "system", "content": main_prompt}]
                    st.session_state.context_data = [{"role": "system", "content": context_promt}]
                    st.session_state.results = ""
                    st.session_state.error_item = ""
                    st.session_state.input_item = ""
                    st.session_state.suggestion = ""
                    st.session_state.wordnotexist = ""
                    st.session_state.impossibleword = ""
                    st.session_state.swearword = ""
                    st.session_state.notenought = ""
                    st.session_state.tokens = 0

                    prompt_contx = ""
                    for i in range(len(st.session_state.keywords)):
                        prompt_contx = prompt_contx + " " + f"{i+1}." + " " + st.session_state.keywords[i].lower().strip()

                    st.session_state.context_data.append({"role": "user", "content": prompt_contx})


                    try:
                        with st.spinner('–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è...'):

                            url = "https://api.theb.ai/v1/chat/completions"
                            res = bot(st.session_state.context_data, url, st.session_state.acsses_key, 1)

                            response = res[0]
                            st.session_state.tokens += res[1]
                            st.session_state.context_data.append({"role": "assistant", "content": response})

                            main_text = response.split("\n\n")
                            words_with_contexts = []

                            for i in range(len(main_text)):
                                contexts1 = main_text[i].split("\n-")[1:]
                                words_with_contexts.append(contexts1)

                            prompt = ""
                            for i in range(len(st.session_state.keywords)):

                                # prompts = {"middle": f'''{i  + 1}.Make up a sentence at elementary English level with the word "{st.session_state.keywords[i].lower()}". Add the context of {contexts[str(random.randint(0, 515))].lower()}. The sentence lenght must be up to 26 words. Do not change the initial form of "{st.session_state.keywords[i].lower()}".'''}
                                prompts = {"middle": f'''{i  + 1}. Word - {st.session_state.keywords[i].lower()}, context - {words_with_contexts[i][random.randint(0, len(words_with_contexts[i])-1)].lower()}.'''}
                                prompt += prompts[st.session_state.level]+"\n"

                            st.session_state.data.append({"role": "user", "content": prompt})

                            url = "https://api.theb.ai/v1/chat/completions"
                            res = bot(st.session_state.data, url, st.session_state.acsses_key, 1)

                            response = res[0]
                            st.session_state.tokens = res[1]
                            st.session_state.data.append({"role": "assistant", "content": response})

                            list_of_sentences = response.split("\n")

                            for word in list_of_sentences:
                                if word == "":
                                    i = list_of_sentences.index(word)
                                    del list_of_sentences[i]

                            status = True
                            for i in range(len(st.session_state.keywords)):

                                item = st.session_state.keywords[i]

                                q = 0
                                data_type = True
                                for j in item.split():
                                    if j in st.session_state.stop_words_v2:
                                        continue
                                    else:
                                        q += 1
                                if q > 1:
                                    data_type = False

                                sentence = list_of_sentences[i]

                                st.session_state.unique_key += 1
                                st.session_state.responses.append(list_of_sentences[i])
                                formated_data = multiple_word_formater(item, sentence, data_type)
                                st.session_state.formated_responses.append([formated_data[0], formated_data[2], formated_data[4], formated_data[1]])

                                if formated_data[3]:
                                    pass
                                else:
                                    status = False

                            if status:
                                gens_number = int(data["gens_number"])
                                updated_value = gens_number - 1
                                database.update({"gens_number": updated_value}, acsses_key)

                                st.session_state.gens_number = updated_value

                            st.session_state.number_of_sentenses = len(st.session_state.responses)

                            random.shuffle(st.session_state.formated_responses)

                            st.session_state.show_submit_form = True
                    except:
                        try:
                            with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è..."):

                                url = "https://api.baizhi.ai/v1/chat/completions"
                                res = bot(st.session_state.context_data, url, st.session_state.acsses_key, 1)

                                response = res[0]
                                st.session_state.tokens += res[1]
                                st.session_state.context_data.append({"role": "assistant", "content": response})

                                main_text = response.split("\n\n")
                                words_with_contexts = []

                                for i in range(len(main_text)):
                                    contexts1 = main_text[i].split("\n-")[1:]
                                    words_with_contexts.append(contexts1)

                                prompt = ""
                                for i in range(len(st.session_state.keywords)):

                                    # prompts = {"middle": f'''{i  + 1}.Make up a sentence at elementary English level with the word "{st.session_state.keywords[i].lower()}". Add the context of {contexts[str(random.randint(0, 515))].lower()}. The sentence lenght must be up to 26 words. Do not change the initial form of "{st.session_state.keywords[i].lower()}".'''}
                                    prompts = {"middle": f'''{i  + 1}. Word - {st.session_state.keywords[i].lower()}, context - {words_with_contexts[i][random.randint(0, len(words_with_contexts[i])-1)].lower()}.'''}
                                    prompt += prompts[st.session_state.level]+"\n"

                                st.session_state.data.append({"role": "user", "content": prompt})

                                url = "https://api.baizhi.ai/v1/chat/completions"
                                res = bot(st.session_state.data, url, st.session_state.acsses_key, 1)
                        
                                response = res[0]
                                st.session_state.tokens = res[1]
                                st.session_state.data.append({"role": "assistant", "content": response})

                                list_of_sentences = response.split("\n")

                                for word in list_of_sentences:
                                    if word == "":
                                        i = list_of_sentences.index(word)
                                        del list_of_sentences[i]

                                status = True
                                for i in range(len(st.session_state.keywords)):

                                    item = st.session_state.keywords[i]
                                    sentence = list_of_sentences[i]

                                    st.session_state.unique_key += 1
                                    st.session_state.responses.append(list_of_sentences[i])
                                    st.session_state.formated_responses.append([formated_data[0], formated_data[2], formated_data[4], formated_data[1]])

                                    if formated_data[3]:
                                        pass
                                    else:
                                        status = False

                                if status:
                                    gens_number = int(data["gens_number"])
                                    updated_value = gens_number - 1
                                    database.update({"gens_number": updated_value}, acsses_key)

                                    st.session_state.gens_number = updated_value

                                st.session_state.number_of_sentenses = len(st.session_state.responses)

                                random.shuffle(st.session_state.formated_responses)

                                st.session_state.show_submit_form = True
                        except:
                            st.error("–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å —Å–µ—Ä–≤–µ—Ä–æ–º –ø—Ä–µ—Ä–≤–∞–Ω–æ, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–æ–∑–¥–∞—Ç—å –∑–∞–¥–∞–Ω–∏–µ –µ—â–µ —Ä–∞–∑.\n –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –ø–æ—è–≤–ª—è—Ç—å—Å—è, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ —Ç–µ—Ö. –ø–æ–¥–¥–µ—Ä–∂–∫—É!", icon="üíî")
                else:

                    if st.session_state.gens_number == 0:
                        st.warning(" &nbsp; –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —á–∏—Å–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –∏—Å—á–µ—Ä–ø–∞–Ω–æ, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—é LexisGen, —á—Ç–æ–±—ã –ø–æ–ø–æ–ª–Ω–∏—Ç—å –µ–≥–æ", icon="üòî")
                    if len(st.session_state.keywords) < 2:
                        st.warning(" &nbsp; –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ö–æ—Ç—è –±—ã 2 —Å–ª–æ–≤–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–¥–∞–Ω–∏—è", icon="üôä")
                    if st.session_state.max_phrase_lenght == False:
                        st.warning(f' &nbsp; –í—ã—Ä–∞–∂–µ–Ω–∏–µ "{st.session_state.too_long_item}" –ø—Ä–µ–≤—ã—à–∞–µ—Ç –¥–æ–ø—É—Å—Ç–∏–º—É—é –¥–ª–∏–Ω–Ω—É. &nbsp; –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–æ–∫—Ä–∞—Ç–∏—Ç–µ —á–∏—Å–ª–æ —Å–ª–æ–≤ ', icon="üòï")
                    if st.session_state.check_language == False:
                        st.warning(f' &nbsp; "{st.session_state.input_item}" —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–æ–≤–∞, –Ω–µ –æ—Ç–Ω–æ—Å—è—â–∏–µ—Å—è –∫ –ê–Ω–≥–ª–∏–π—Å–∫–æ–º—É —è–∑—ã–∫—É', icon="üòë")
                    if st.session_state.propriate_spell == "impossible_word" and st.session_state.check_language:
                        st.warning(f' &nbsp; –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –Ω–µ —Å–º–æ–≥–ª–∞ –¥–æ–≥–∞–¥–∞—Ç—å—Å—è, —á—Ç–æ –í—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É –ø–æ–¥ "{st.session_state.impossibleword}"...', icon="üòµ")
                    if st.session_state.propriate_spell == False:
                        st.warning(f'  &nbsp; "{st.session_state.wordnotexist}" —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—à–∏–±–∫–∏. –í–æ–∑–º–æ–∂–Ω–æ, –í—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É "{st.session_state.suggestion}"?', icon="‚ö†Ô∏è")
                    if st.session_state.not_in_sw == False:
                        st.warning(f'  &nbsp; –¢–æ–ª—å–∫–æ "{st.session_state.notenought}" –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è...', icon="üòï")
                    if st.session_state.profanity:
                        st.warning(f' &nbsp; "{st.session_state.swearword}" —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—É—é –∏–ª–∏ –Ω–µ—Ü–µ–Ω–∑—É—Ä–Ω—É—é –ª–µ–∫—Å–∏–∫—É!', icon='üò∂')
            else:
                st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∫–ª—é—á –¥–æ—Å—Ç—É–ø–∞", icon="üîë")

    if st.session_state.show_submit_form:
        # st.info('–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö –Ω–∏–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–æ—Ä–º–æ–π —Å–ª–æ–≤–∞ –∏–∑ —Å–ø–∏–∫–∞ –≤—ã—à–µ.')
        # st.info('–í–≤–µ–¥–µ–Ω–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö –Ω–∏–∂–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ.')
        # st.info('–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö –Ω–∏–∂–µ.')

        with st.form("my_form"):

            if len(st.session_state.answers) == 0:
                for answer in st.session_state.formated_responses:
                    st.session_state.answers.append(answer[1])

            
            for i in range(st.session_state.number_of_sentenses):
                try:
                    st.session_state.user_answers[i] = input_field(st.session_state.formated_responses[i][0], key=st.session_state.unique_key + i)
                    st.session_state.user_answers[i] = " ".join(st.session_state.user_answers[i])
                except:
                    pass

            if st.form_submit_button("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å"):
                try:
                    st.session_state.results = ""
                    for i in range(len(st.session_state.user_answers)):
                        if len(st.session_state.user_answers[i]) != 0:
                            if re.sub(r'[^\-\'\w\s]', '', st.session_state.user_answers[i].lower().strip()) == re.sub(r'[^\-\'\w\s]', '', st.session_state.answers[i].lower().strip()):
                                st.session_state.results += f" ‚úîÔ∏è :green[{st.session_state.user_answers[i].lower().strip()}]"
                            elif len(st.session_state.user_answers[i][0].strip()) > 0:
                                st.session_state.results += f" ‚ùå :red[{st.session_state.user_answers[i].lower()}]"

                    st.write(st.session_state.results)
                except:
                    pass

        with st.expander("–û—Ç–≤–µ—Ç—ã"):
            for i in range(st.session_state.number_of_sentenses):
                try:
                    correct_sentence = st.session_state.formated_responses[i][2]
                    correct_sentence = correct_sentence.split()
                    for item in st.session_state.answers[i].split():
                        for i in range(len(correct_sentence)):
                            if correct_sentence[i] == "[G_A_P]":
                                correct_sentence[i] = f":blue[{item}]"
                                break

                    correct_sentence = " ".join(correct_sentence)
                    st.write(correct_sentence)
                except:
                    pass

        # for i in range(st.session_state.number_of_sentenses):
        #     st.write(st.session_state.formated_responses[i][3])




main_grid.write("")

with st.sidebar:
    if st.session_state.key_is_provided:
        st.title(f"–û—Å—Ç–∞–ª–æ—Å—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–π: {st.session_state.gens_number}")


    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")
    st.write("")


    st.link_button("‚ö° –ü–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø", "https://vk.com/@lexisgen-kak-poluchit-dostup")
    st.link_button("üîó –°–æ–æ–±—â–µ—Å—Ç–≤–æ –≤ –í–ö", "https://vk.com/lexisgen")
    st.link_button("‚öôÔ∏è –¢–µ—Ö. –ø–æ–¥–¥–µ—Ä–∂–∫–∞ &nbsp; &nbsp;", "https://vk.com/lostconcepts")